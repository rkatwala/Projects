# Basketball BPE Tokenization

This repository demonstrates how to train a simple Byte-Pair Encoding (BPE) tokenizer on a small “basketball facts” corpus and then apply it to tokenize text. All code is pure Python and does not rely on external libraries beyond the standard library.

## Repository Contents

- **`basketball_sample.txt`**  
  A text file containing 100 lines of “Basketball Fact #n: …” statements (one per line).  
- **`bpe_train.py`**  
  Python script that:
  1. Regenerates `basketball_sample.txt` (100 basketball facts).
  2. Trains a BPE model with a specified number of merges.
  3. Saves:
     - `bpe_merges.txt` – the ordered list of merge operations.
     - `bpe_vocab.txt` – final vocabulary (subword sequences + frequencies).
  4. Tokenizes the first line of `basketball_sample.txt` using the learned merges and prints the subword splits.
- **`bpe_merges.txt`**  
  (Generated by running `bpe_train.py`)  
  Each line contains a pair of symbols that were merged in order, e.g.  
```

b a
ba s
...

````
- **`bpe_vocab.txt`**  
(Generated by running `bpe_train.py`)  
Lists all final subword sequences (as space-separated symbols) with their corpus frequencies, sorted by descending frequency.

## How It Works

1. **Build Initial Vocabulary**  
 - Read each line of `basketball_sample.txt`, split on whitespace, and append a special end-of-word marker `</w>` to every word.  
 - Track how many times each “character sequence + `</w>`” appears.

2. **Count Symbol Pairs**  
 - For each vocabulary entry, count all adjacent symbol pairs (e.g., `(b, a)`, `(a, s)`, etc.) across all words, weighting by frequencies.

3. **Merge Most Frequent Pair**  
 - Identify the single most frequent adjacent pair.
 - Replace every occurrence of that pair (in every word) with a new merged symbol (concatenate the two symbols, e.g. `"ba"`).  
 - Repeat up to `NUM_MERGES` times (500 by default).

4. **Save Merge Rules & Final Vocabulary**  
 - `bpe_merges.txt`: ordered list of all `(symbol1, symbol2)` pairs merged, one per line.  
 - `bpe_vocab.txt`: each line contains a final BPE token (one or more characters possibly fused) followed by its frequency.

5. **Tokenize Example Sentence**  
 - Take the first line of `basketball_sample.txt` (`"Basketball Fact #1: The first game …"`).  
 - For each word:
   - Lowercase it, split into individual characters + `</w>`.  
   - Iteratively apply each merge rule in the same order as training—whenever two adjacent symbols match a merge pair, fuse them—until no more merges apply.  
   - Remove the `</w>` marker to produce final subword tokens.  
 - Print each original word alongside its list of BPE subword tokens.

---

## Prerequisites

- Python 3.7+ (no additional packages required)

---

## Usage

1. **Clone or download** the repository, then **`cd`** into it:
 ```bash
 git clone <repo-url>
 cd basketball-bpe
````

2. **Run the training script**:

   ```bash
   python bpe_train.py
   ```

   This will:

   * Recreate `basketball_sample.txt` with 100 basketball facts.
   * Train a BPE model with 500 merges (adjustable in the script).
   * Produce:

     * **`bpe_merges.txt`** – merge operations (one pair per line).
     * **`bpe_vocab.txt`** – final BPE vocabulary with frequencies.
   * Tokenize and print the subword splits for the first line of the corpus.

3. **Inspect outputs**:

   * Open **`bpe_merges.txt`** to see how symbols were merged step by step.
   * Open **`bpe_vocab.txt`** to see the final subword units and their frequencies.
   * Check your console output for the tokenization example.

---

## Adjusting Parameters

* **Number of Merges**
  In `bpe_train.py`, modify:

  ```python
  NUM_MERGES = 500
  ```

  to increase or decrease how many merge operations are performed. More merges yield a larger subword vocabulary.

* **Corpus File**
  By default, `bpe_train.py` regenerates `basketball_sample.txt`. If you want to train on a different text file, replace the contents of `basketball_sample.txt` with your own corpus (one sentence per line) and rerun the script.

---

## Example Output (First Line Tokenization)

After running `python bpe_train.py`, you should see something like:

```
Loaded 100 unique word tokens from the corpus.
Performed 500 merge operations.
Saved merge rules to 'bpe_merges.txt'.
Saved final BPE vocabulary to 'bpe_vocab.txt'.

First line: Basketball Fact #1: The first game of basketball was played with a soccer ball.

Tokenization (each word -> list of BPE subwords):
Basketball  →  ['basketball</w>']
Fact        →  ['fact</w>']
#1:         →  ['#1', ':</w>']
The         →  ['the</w>']
first       →  ['first</w>']
game        →  ['game</w>']
of          →  ['of</w>']
basketball  →  ['basketball</w>']
was         →  ['was</w>']
played      →  ['played</w>']
with        →  ['with</w>']
a           →  ['a</w>']
soccer      →  ['s', 'occ', 'er</w>']
ball.       →  ['ba', 'll', '.</w>']
```

---

## Directory Structure

```
basketball-bpe/
├── basketball_sample.txt    # Generated corpus of 100 basketball facts
├── bpe_merges.txt           # Merge rules learned during training
├── bpe_vocab.txt            # Final BPE token sequences + frequencies
├── bpe_train.py             # Main training + tokenization script
└── README.md                # This documentation file
```

---

## License

This code is provided under MIT License. Feel free to reuse or modify for your own purposes.
